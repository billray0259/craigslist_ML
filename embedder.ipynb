{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "dataset_id = \"20220329222400\"\n",
    "\n",
    "dataset_dir = os.path.join(data_dir, dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text, images, batch_size=100):\n",
    "    all_text_embeds = []\n",
    "    all_image_embeds = []\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    pixel_values_list = []\n",
    "    for i in tqdm(range(0, max(len(images), len(text)), batch_size)):\n",
    "        text_batch = text[i:i+batch_size] if i+batch_size < len(text) else text[i:]\n",
    "        image_batch = images[i:i+batch_size] if i+batch_size < len(images) else images[i:]\n",
    "\n",
    "        inputs = processor(text=text_batch, images=image_batch, return_tensors=\"pt\", padding=True)\n",
    "        input_ids = inputs[\"input_ids\"][:, :77]\n",
    "        attention_mask = inputs[\"attention_mask\"][:, :77]\n",
    "        pixel_values = inputs[\"pixel_values\"]\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        # outputs = model(**inputs)\n",
    "        text_embeds = outputs.text_embeds\n",
    "        image_embeds = outputs.image_embeds\n",
    "        # get as numpy array\n",
    "        text_embeds = text_embeds.detach().cpu().numpy()\n",
    "        image_embeds = image_embeds.detach().cpu().numpy()\n",
    "\n",
    "        all_text_embeds.append(text_embeds)\n",
    "        all_image_embeds.append(image_embeds)\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        pixel_values_list.append(pixel_values)\n",
    "\n",
    "    all_text_embeds = np.concatenate(all_text_embeds, axis=0)\n",
    "    all_image_embeds = np.concatenate(all_image_embeds, axis=0)\n",
    "\n",
    "    input_ids_list = np.concatenate(input_ids_list, axis=0)\n",
    "    attention_mask_list = np.concatenate(attention_mask_list, axis=0)\n",
    "    pixel_values_list = np.concatenate(pixel_values_list, axis=0)\n",
    "\n",
    "    return all_text_embeds, all_image_embeds, (input_ids_list, attention_mask_list, pixel_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b27594adc9433994b64df368aaaa18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_file = os.path.join(dataset_dir, \"detailed.json\")\n",
    "\n",
    "# read search_file\n",
    "with open(search_file, \"r\") as f:\n",
    "    search = json.load(f)\n",
    "\n",
    "texts = []\n",
    "images = []\n",
    "prices = []\n",
    "ids = []\n",
    "\n",
    "for post_id, post in tqdm(search.items()):\n",
    "    title = post[\"name\"]\n",
    "    description = post[\"description\"]\n",
    "    text = f\"{title}\\n{description}\"\n",
    "    texts.append(text)\n",
    "\n",
    "    image_file = os.path.join(dataset_dir, \"images\", post_id + \".jpg\")\n",
    "    image = Image.open(image_file)\n",
    "    images.append(image)\n",
    "\n",
    "    price = post[\"price\"]\n",
    "    prices.append(price)\n",
    "\n",
    "    ids.append(post_id)\n",
    "\n",
    "prices = np.array(prices)\n",
    "ids = np.array(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f1e99edcea4e34829713c18a59fc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (128 > 77). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text_embeds, image_embeds, inputs = get_embeddings(texts, images)\n",
    "# save embeddings as text_embeddings.npy and image_embeddings.npy and prices and ids\n",
    "np.save(os.path.join(dataset_dir, \"text_embeds.npy\"), text_embeds)\n",
    "np.save(os.path.join(dataset_dir, \"image_embeds.npy\"), image_embeds)\n",
    "np.save(os.path.join(dataset_dir, \"prices.npy\"), prices)\n",
    "np.save(os.path.join(dataset_dir, \"ids.npy\"), ids)\n",
    "\n",
    "input_ids, attention_mask, pixel_values = inputs\n",
    "np.save(os.path.join(dataset_dir, \"input_ids.npy\"), input_ids)\n",
    "np.save(os.path.join(dataset_dir, \"attention_mask.npy\"), attention_mask)\n",
    "np.save(os.path.join(dataset_dir, \"pixel_values.npy\"), pixel_values)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cce4bdf21c3861b97cbca96bad86e52b4e5bfa8ac1d28e8dfe7fd4ceb671f40e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
